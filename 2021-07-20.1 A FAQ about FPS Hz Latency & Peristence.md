# A FAQ (of sorts) about FPS, Hz, Latency, and Persistence

These are all things that have been said to me over the years. I didn't necessarily have the reply ready to go at the time, but I never stopped thinking about how I should have answered. Yes, this is a rehearsed shower argument presented in writing.  
Jokes aside, this is an area of intense curiosity for me and I'm always eager to demystify it for whoever asks. I am by no means an expert on the subject and here I will only be addressing some core concepts in a casual manner. If you want to gain a deeper understanding of what I discuss here, the blurbusters blog is an excellent resource: <https://blurbusters.com/category/area51-display-research/>

Keep in mind, vision is a complicated process that is constructed by your brain. We do not operate like cameras. A decent sample of the complexity of vision can be found here <https://www.pcgamer.com/how-many-frames-per-second-can-the-human-eye-really-see/>.

Despite the "statement/response" structure I've chosen, this is meant to be read in its entirety. I attempted to keep answers somewhat brief, and to have later answers build upon concepts discussed in previous answers.

## The "FAQ"

> "Higher framerate only matters for fast paced shooter games, it doesn't benefit me because I play other kinds of games."

The unstated premise: "there's only a mechanical benefit, and the benefit is strictly about player performance." Yes, low latency is important for competitive play, and high framerates reduce latency, but framerate in of itself has a qualitative benefit separate from reducing latency. Arguably, it matters more for "slower" games! Framerate is distinct from Hz is distinct from latency.


> "I can't see the difference between 30 and 60, or 60 and 120"

If you're curious and want suggestions for what to look at, begin with comparing high and low fps for "slower" games; particularly ones where the entire frame moves slowly and steadily. The difference is more pronounced in some contexts more than others. Also check out [these][1] [two][2] browser demonstrations.


> "120 was a huge qualitative bump over 60, but any higher offers diminishing returns."

Anything over 120fps? Probably. Over 120Hz on a sample&hold display? Well, it depends. Without some form of blur reduction present, blur is a function of Hz. Higher Hz confers less blur. There will always be contexts where any amount of blur is quite noticeable and some form of mitigation is desired.  
As an aside about technology advancements and "diminishing returns", often enough technology evolves in unforeseen directions. Invariably something new comes along and fills the possibility space; something which was not previously possible with the prior constraints. Although perhaps that's not indicative of how much you'll care about the hypothetical new thing :)


> "Why did 30fps suddenly become a problem when it never used to be? There were plenty of 30fps games in the past and no one complained- in fact, Ocarina of Time was 20fps! Now that you have highfps, **you** just think anything less looks worse."

The problem is with ["persistence of vision"][3], a factor minimally present on CRTs but is a large thorn in modern sample&hold displays. I'll say it again: 30fps was never the problem, _persistence_ is the problem. Furthermore, when viewer attention is fixed to center screen, there's less opportunity for "persistence of vision" blurring to occur. Thanks to a lack of analog camera control (and with Ocarina of Time in particular, z-targeting) many such old games hold up remarkably well on our modern sample&hold displays. The same saving graces are not guaranteed present in modern games, however. :(


> "make up some question here I guess, to split up this dumptruck subject"

Consider the following:

- A 0fps static picture looks just fine on a sample&hold display.
- A lowfps scene where the camera is static but has tiny objects moving within it will also render just fine, because blur is restricted to tiny objects- and even then only if your eye is actively tracking those objects. The object's size, speed, and distance-traveled are all factors that influence how much blur we perceive. The size of your display and your viewing distance may also be factors!

Take a look at these pixel art animations (artist [1041uuu on tumblr](https://1041uuu.tumblr.com))  
![6fps](https://ghcdn.rawgit.org/AeliusSaionji/aeliussaionji.github.io/bc2563e6af7a2b351a0f73c97247c6217d1fa4a1/2021-07-20.1%20A%20FAQ%20about%20FPS%20Hz%20Latency%20%26%20Peristence/1041uuu_6fps.gif)  
<https://1041uuu.tumblr.com/post/149361423843>  
![8fps](https://ghcdn.rawgit.org/AeliusSaionji/aeliussaionji.github.io/bc2563e6af7a2b351a0f73c97247c6217d1fa4a1/2021-07-20.1%20A%20FAQ%20about%20FPS%20Hz%20Latency%20%26%20Peristence/1041uuu_8fps.gif)  
<https://1041uuu.tumblr.com/post/637105352644706304>  

To my eye, these look great, even on a sample&hold display. They are 6fps and 8fps, respectively. In both, each object has minimal motion, and there are many moving objects. Therefore, no one object has much blur, and my attention is not focused on any specific object thanks to the complexity of the scene. Lowfps is not _always_ a problem; although the point at which it becomes a problem is, I'm sure, a subjective measure.  
But when the entire frame is in motion - such as when panning the camera - the entire frame will have blur. Clearly, the **extent to which this is a problem is highly contextual**. In a first person shooter the impact may not necessarily be dramatic; much of the "tracking" involves centering an object on-screen such that it is technically unmoving with respect to the display (not unlike Zelda's z-targeting). Tracking an object with the mouse/joystick instead of your eyes neatly sidesteps the problem.  
More jarring contexts occur with a 3rd person camera which the player is allowed to freely pivot around the player character. Frequently the play of these games involve positioning the camera such that an object is visible, but not necessarily unmoving with respect to the display. Tracking an object with your eye while it moves within the frame, while the entire frame itself is also moving, creates blur upon blur that can be quite offensive to the eye.
Another context where heavy "persistence" occurs frequently is with 2D indie games: any time the camera slowly pans across a static scene!


> "What makes CRTs so special?"

Here's a simple analogy that roughly models the relationship between a given display and its Hz:
```
 LCD,OLED 60Hz : eyeballs :: 1/60   shutter speed : film
LCD,OLED 120Hz : eyeballs :: 1/120  shutter speed : film
      CRT 60Hz : eyeballs :: 1/1000 shutter speed : film
     CRT 120Hz : eyeballs :: 1/1000 shutter speed : film
```
On our modern displays persistence is function of Hz, but only if there's a unique frame per Hz. 30fps on a 60Hz display has persistence functionally equivalent to 30Hz or a 1/30 shutter speed! To get the persistence equivalent to 1/60 shutter on a 60Hz display, we need to be pushing 60fps.  
Stated from another angle: higher fps requires higher Hz to render, which inherently decreases the effect of persistence.  
Note: if it wasn't clear by now, CRT is _not_ sample&hold.  
Note2: if you're curious about plasma's effective "shutter speed", it sits notably slower than CRT, but notably faster than sample&hold. I can't be more specific because I've no idea how to quantify the complicated way in which plasma works into a simple number.  
Note3: Blur reduction exists for sample&hold displays. The goal is, in the terms of our analogy, to "increase the shutter speed of the display". Unfortunately not all blur reduction implementations actually reach a 1/1000; perhaps manufacturers are afraid of buyers complaining about flicker, or the reduction in brightness.


> "30fps is fine because movies are shot in 24fps and they're fine!"

Benefits of lower latency aside, the problem with this argument is that it equates twiddling an analog stick with how a trained director chooses to frame a given scene, and how a trained camera crew follows a given shot. Movies are shot with careful intentionality, with awareness of the confines of the medium. Furthermore, a real world camera can capture temporal information beyond its framerate in the form of blur: simply adjust the shutter speed. By contrast, a videogame is typically rendering instantaneous moments in time, which by itself exacerbates the problem.


[1]: https://testufo.com/framerates#count=2&background=none&pps=240
[2]: https://testufo.com/framerates-versus#photo=bumblebee.jpg&pps=960&framepacingerror=0&direction=rtl&framerate=30&compare=2&showfps=1
[3]: https://blurbusters.com/gtg-versus-mprt-frequently-asked-questions-about-display-pixel-response/
