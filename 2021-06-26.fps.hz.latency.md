# A FAQ (of sorts) about fps, hz, latency, and persistence.

These are all things that have been said to me over the years. I didn't necessarily have the reply ready to go at the time, but I never stopped thinking about how I should have answered- yes, this is a rehearsed shower argument presented in writing.
Joking aside, this is an area of intense curiosity for me and I'm always eager to demystify it for whoever asks. I am by no means an expert on the subject and here I will only be addressing some core concepts in a casual manner. If you want to gain a deeper understanding of what I discuss here, the blurbusters blog is an excellent resource: <https://blurbusters.com/category/area51-display-research/>

Keep in mind, vision is a complicated process that is constructed by your brain. We do not operate like cameras. A decent sample of the complexity of vision can be found here <https://www.pcgamer.com/how-many-frames-per-second-can-the-human-eye-really-see/>.

Despite the "statement/response" structure I've chosen, this is meant to be read in its entirety. I attempted to keep answers somewhat brief, and to have later answers build upon concepts discussed in previous answers.

## The "FAQ"

> "Higher framerate only matters for fast paced shooter games, it doesn't benefit me because I play other kinds of games."
The unstated premise: there's only a mechanical benefit, and the benefit is strictly about player performance. Yes, low latency is important for competitive play, and high framerates reduce latency, but framerate in of itself has a qualitative benefit separate from reducing latency. Arguably, it matters more for "slower" games! Framerate is distinct from Hz is distinct from latency.

> "I can't see the difference between 30 and 60, or 60 and 120"
Try comparing with "slower" games rather than shooters; particularly ones where the entire frame moves slowly and steadily. The difference is more pronounced in some contexts more than others.

> "120 was a huge qualitative bump over 60, but any higher is diminishing returns."
Anything over 120fps? Yeah probably. Over 120Hz on a sample&hold display? Well, it depends. Without some form of blur reduction present, blur is a function of Hz. Higher Hz confers less blur. There will always be contexts where any amount of blur is quite noticeable and some form of mitigation is desired.

> "Why did 30fps suddenly become a problem when it never used to be? There were plenty of 30fps games in the past and no one complained- in fact, Ocarina of Time was 20fps! Now that you have highfps, **you** just think anything less looks worse."
The problem is with ["persistence of vision"][1], a factor minimally present on CRTs but is a large thorn in modern sample&hold displays. I'll say it again: 30fps was never the problem, _persistence_ is the problem. Furthermore, when viewer attention is fixed to center screen, there's less opportunity for "persistence of vision" blurring to occur. Thanks to a lack of analog camera control (and with Ocarina of Time in particular, z-targeting) many such old games hold up remarkably well on our modern sample&hold displays. The same saving graces are not guaranteed present in modern games, however...

> "make up some question here I guess, to split up this dumptruck subject"
A 0fps static picture looks just fine on a sample&hold display.
A lowfps scene where the camera is static but has tiny objects moving within it will also render just fine, because blur is restricted to tiny objects- and even then only if your eye is actively tracking those objects. The object's size, speed, and distance-traveled are all factors that influence how much blur we perceive. The size of your display and your viewing distance may also be factors!
- Present lowfps where persistence isn't a problem.
- Note that the point at which is becomes "a problem" is subjective.
https://1041uuu.tumblr.com/post/149361423843 6fps
https://1041uuu.tumblr.com/post/637105352644706304 8fps
But when the entire frame is in motion - such as when panning the camera - the entire frame will have blur. Clearly, the **extent to which this is a problem is highly contextual**. In a first person shooter the impact may not necessarily be dramatic; much of the "tracking" involves centering an object on-screen such that it is technically unmoving with respect to the display (not unlike Zelda's z-targeting). Tracking an object with the mouse/joystick instead of your eyes neatly sidesteps the problem.
More jarring contexts occur with a 3rd person camera which the player is allowed to freely pivot around the player character. Frequently the play of these games involve positioning the camera such that an object is visible, but not necessarily unmoving with respect to the display. Tracking an object with your eye while it moves within the frame, while the entire frame itself is also moving, creates blur upon blur that can be quite offensive to the eye.
Another context where heavy "persistence" occurs frequently with 2D indie games: any time the camera slowly pans across a static scene!

> "What makes CRTs so special?"
Here's a simple analogy that roughly models the relationship between a given display and its Hz:
```
 LCD,OLED 60Hz : eyeballs :: 1/60   shutter speed : film
LCD,OLED 120Hz : eyeballs :: 1/120  shutter speed : film
      CRT 60Hz : eyeballs :: 1/1000 shutter speed : film
     CRT 120Hz : eyeballs :: 1/1000 shutter speed : film
```
This is why lowfps impacts our modern sample&hold displays more than CRT. Persistence has become a function of Hz, but only if there's a unique frame per Hz. 30fps on a 60Hz display has persistence functionally equivalent to 30Hz or a 1/30 shutter speed! To get the persistence equivalent to 1/60 shutter on a 60Hz display, we need to be pushing 60fps.
Stated from another angle: higher fps requires higher Hz to render, which inherently decreases the effect of persistence.
Note: if it wasn't clear by now, CRT is _not_ sample&hold.
Note2: if you're curious about plasma's effective "shutter speed", it sits notably slower than CRT, but notably faster than sample&hold. I can't be more specific because I've no idea how to quantify the complicated way in which plasma works into a simple number.
Note3: Blur reduction exists for sample&hold displays. The goal is, in effect, to "increase the shutter speed of the display" (quotes because this is strictly an analogy). Not all blur reduction implementations actually reach a 1/1000; perhaps manufacturers are afraid of buyers complaining about flicker, or the reduction in brightness.


> "30fps is fine because movies are shot in 24fps and they're fine!"
Benefits of lower latency aside, the problem with this argument is that it equates twiddling an analog stick with how a trained director chooses to frame a given scene, and how a trained camera crew follows a given shot. Movies are shot with careful intentionality, with awareness of the confines of the medium. Furthermore, a real world camera can capture temporal information beyond its framerate in the form of blur: simply adjust the shutter speed. By contrast, a videogame is typically rendering instantaneous moments in time, which by itself exacerbates the problem.


[1]: https://blurbusters.com/gtg-versus-mprt-frequently-asked-questions-about-display-pixel-response/
